from pathlib import Path
import json
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset

from data_preprocessing import KubernetesPromptBuilder
from LLM_executor import LLMExecutor
from metrics.solution_metrics import SolutionMetrics
import re
from collections import Counter

current_dir = Path(__file__).resolve().parent
json_dir = current_dir.parent / "crash-cases" / "hardcoded-database"
answers_dir = current_dir.parent / "crash-cases" / "ground-truth-answers"
results_dir = current_dir / "results"
results_dir.mkdir(exist_ok=True)
deepeval_results_dir = results_dir / "deepeval"
deepeval_results_dir.mkdir(exist_ok=True)

def run_deepeval_comparison():
    """
    Compare error explanation results with ground-truth using DeepEval
    """
    print("\n=== Executing result comparison using DeepEval ===\n")
    
    # Load results generated by the model
    results_path = results_dir / "error_explanation_from_answers.json"
    
    if not results_path.exists():
        print(f"Error: Results file {results_path} not found!")
        return
    
    with open(results_path, "r", encoding="utf-8") as f:
        error_explanations = json.load(f)
    
    all_test_cases = []
    evaluation_results = []
    
    # Define metrics
    relevancy_metric = AnswerRelevancyMetric(model="gpt-3.5-turbo")
    solution_metrics = SolutionMetrics()
    
    # Process each case
    for explanation_result in error_explanations:
        case_name = explanation_result["case"]
        llm_output = explanation_result["explanation"]
        
        # Find corresponding ground-truth file
        gt_file = answers_dir / f"ANS-{case_name}.json"
        if not gt_file.exists():
            print(f"Warning: No ground-truth found for {case_name}")
            continue
        
        with open(gt_file, "r", encoding="utf-8") as f:
            ground_truth = json.load(f)
        
        # Process ground-truth solution depending on format
        gt_solution = ground_truth["solution"]
        if isinstance(gt_solution, list):
            gt_solution_text = "\n".join(gt_solution)
        else:
            gt_solution_text = gt_solution
        
        # Prepare DeepEval test case
        test_case = LLMTestCase(
            input=f"Explain Kubernetes problem: {case_name}",
            actual_output=llm_output,
            expected_output=gt_solution_text,
            retrieval_context=ground_truth["documentation"]
        )
        all_test_cases.append(test_case)
        
        # Calculate results for single case
        try:
            # Answer relevancy using DeepEval
            relevancy_score = relevancy_metric.measure(test_case)
            
            # Command exactness using SolutionMetrics
            command_exactness_score = solution_metrics.command_exactness(gt_solution_text, llm_output)
            
            # F1 score over technical words
            f1_technical_score = solution_metrics.f1_over_words_technical(llm_output, gt_solution_text)
            
            # Hallucination detection - extract steps for comparison
            actual_steps = [step.strip() for step in re.split(r'\d+\.', llm_output) if step.strip()]
            allowed_steps = [step.strip() for step in re.split(r'\d+\.', gt_solution_text) if step.strip()]
            hallucination_score = solution_metrics.kubernetes_hallucination_detection(actual_steps, allowed_steps)
            
        except Exception as e:
            print(f"Error when calculating metrics: {e}")
            relevancy_score = 0.5  # Default values in case of error
            command_exactness_score = 0.0
            f1_technical_score = 0.0
            hallucination_score = 0.0
        
        case_result = {
            "case": case_name,
            "relevancy_score": relevancy_score,
            "command_exactness": command_exactness_score,
            "f1_technical": f1_technical_score,
            "hallucination_score": hallucination_score
        }
        evaluation_results.append(case_result)
        
        print(f"Results for {case_name}:")
        print(f"  Answer relevancy: {relevancy_score:.4f}")
        print(f"  Command exactness: {command_exactness_score:.4f}")
        print(f"  F1 technical: {f1_technical_score:.4f}")
        print(f"  Hallucination score: {hallucination_score:.4f}")
    
    # Calculate average metrics
    if evaluation_results:
        avg_relevancy = sum(r["relevancy_score"] for r in evaluation_results) / len(evaluation_results)
        avg_command_exactness = sum(r["command_exactness"] for r in evaluation_results) / len(evaluation_results)
        avg_f1_technical = sum(r["f1_technical"] for r in evaluation_results) / len(evaluation_results)
        avg_hallucination = sum(r["hallucination_score"] for r in evaluation_results) / len(evaluation_results)
    else:
        avg_relevancy = 0.0
        avg_command_exactness = 0.0
        avg_f1_technical = 0.0
        avg_hallucination = 0.0
        print("Warning: No results to analyze")
    
    # Save full results
    summary_results = {
        "individual_results": evaluation_results,
        "average_metrics": {
            "relevancy_score": avg_relevancy,
            "command_exactness": avg_command_exactness,
            "f1_technical": avg_f1_technical,
            "hallucination_score": avg_hallucination
        }
    }
    
    with open(deepeval_results_dir / "comparison_results.json", "w", encoding="utf-8") as f:
        json.dump(summary_results, f, indent=2)
    
    # Execute evaluation as a dataset with DeepEval
    try:
        dataset = EvaluationDataset(test_cases=all_test_cases)
        dataset_results = evaluate(dataset, [relevancy_metric])
        
        # Create a serializable version of the dataset_results
        serializable_results = {
            "test_results": [
                {
                    "name": result.name,
                    "success": result.success,
                    "metrics_data": [
                        {
                            "name": metric.name,
                            "threshold": metric.threshold,
                            "success": metric.success,
                            "score": metric.score,
                            "reason": metric.reason,
                            "strict_mode": metric.strict_mode,
                            "evaluation_model": metric.evaluation_model,
                            "error": metric.error
                        } for metric in (result.metrics_data or [])
                    ]
                } for result in dataset_results.test_results
            ],
            "confident_link": dataset_results.confident_link
        }
        
        with open(deepeval_results_dir / "dataset_results.json", "w", encoding="utf-8") as f:
            json.dump(serializable_results, f, indent=2)
    except Exception as e:
        print(f"Error during dataset evaluation: {e}")
        print("Only individual results were saved.")
    
    print("\nAverage Metrics:")
    print(f"  Answer relevancy: {avg_relevancy:.4f}")
    print(f"  Command exactness: {avg_command_exactness:.4f}")
    print(f"  F1 technical: {avg_f1_technical:.4f}")
    print(f"  Hallucination score: {avg_hallucination:.4f}")
    print(f"\nComparison results saved in {deepeval_results_dir}")
    
    return summary_results

if __name__ == "__main__":
    # Run DeepEval comparison
    run_deepeval_comparison()
