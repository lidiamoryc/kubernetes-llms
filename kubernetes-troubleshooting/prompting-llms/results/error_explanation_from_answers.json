[
  {
    "case": "ReadinessProbeFailed",
    "documentation": "1. Readiness Probes and HTTP Status Codes \n A readiness probe checks if the application is ready to serve requests. If the probe returns a failure (e.g., 404), the container is marked as not ready and removed from the service's endpoints.\n\n2. Proper Configuration of Readiness Probes \n The HTTP probe must return a status code between 200 and 399 to be considered successful. A 404 status code is treated as a failure.\n\n3. Serving Readiness Probe Paths \n The application must expose an HTTP server and listen on the defined port and path. Ensure the endpoint (e.g., `/healthz`) exists and returns an acceptable status code.",
    "explanation": "The root cause of the issue is that the probe timeout might be too short for database dependencies, leading to readiness probe failures.\n\n1. Update Readiness Probe Configuration\n\nEdit the deployment YAML file for the application and adjust the readiness probe configuration to allow more time for the probe to complete.\n\n# Before\nreadinessProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 10\n  timeoutSeconds: 1\n\n# After\nreadinessProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 10\n  timeoutSeconds: 5\n\n2. Verify Application Readiness\n\nCheck the logs of the application pod to ensure that the readiness probe is successful and the application is ready to serve requests.\n\nkubectl logs <pod_name>\n\nLook for any errors related to the readiness probe or application startup.\n\n3. Test Readiness Probe\n\nExecute a manual readiness probe test to verify that the application responds correctly.\n\nkubectl exec -it <pod_name> -- curl localhost:8080/healthz\n\nEnsure that the response status code is between 200 and 399, indicating a successful readiness probe.\n\n4. Monitor Readiness Probe Status\n\nMonitor the readiness probe status over time to ensure that the application remains ready and responsive to requests.\n\nkubectl describe pod <pod_name>\n\nLook for the conditions section to check the status of the readiness probe and any related events.\n\nBy following these steps, you can adjust the readiness probe configuration, verify application readiness, test the probe manually, and monitor the probe status to ensure that the application is ready to serve requests without any issues."
  },
  {
    "case": "CreateContainerConfigError",
    "documentation": "1. ConfigMap Mounting Requirements \n When mounting a ConfigMap key as a file using subPath, the key must exist in the ConfigMap's data field. If the key is missing or the ConfigMap doesn't exist, the mount fails with CreateContainerConfigError.\n\n2. Error Cause Identification \n CreateContainerConfigError occurs when a Pod references a non-existent ConfigMap. Verify the ConfigMap exists in the same namespace as the Pod with kubectl get configmap <name>.\n\n3. Proper Volume Configuration \n To mount a single ConfigMap key as a file without items, the entire ConfigMap is mounted as a directory, not individual files.",
    "explanation": "The root cause of the issue is that the Pod is failing to read the config file at /app/config.json due to a missing or non-existent ConfigMap key.\n\n1. Verify ConfigMap Key Existence\n   - Run the following command to check if the key \"config.json\" exists in the ConfigMap:\n     ```\n     kubectl get configmap <configmap-name> -n <namespace>\n     ```\n   - Look for the key \"config.json\" in the output. If it's missing, proceed to the next step.\n\n2. Create or Update ConfigMap Key\n   - If the key is missing, create or update the ConfigMap with the key \"config.json\" and the corresponding data.\n     ```\n     kubectl create configmap <configmap-name> --from-file=config.json=/path/to/config.json -n <namespace>\n     ```\n   - This command creates a ConfigMap with the key \"config.json\" and the data from the file at the specified path.\n\n3. Mount ConfigMap Key in Pod\n   - Update the Pod's volume and volumeMounts to mount the ConfigMap key as a file at the correct path.\n     ```yaml\n     volumes:\n     - name: config-volume\n       configMap:\n         name: <configmap-name>\n     ...\n     containers:\n     - name: <container-name>\n       volumeMounts:\n       - name: config-volume\n         mountPath: /app\n         subPath: config.json\n     ```\n   - This configuration mounts the \"config.json\" key from the ConfigMap as a file at the path /app/config.json in the Pod.\n\n4. Restart the Pod\n   - Delete and recreate the Pod to apply the changes and ensure it reads the config file successfully.\n     ```\n     kubectl delete pod <pod-name> -n <namespace>\n     ```\n     ```\n     kubectl create -f pod-definition.yaml -n <namespace>\n     ```\n   - This will restart the Pod with the updated configuration to read the config file from the mounted ConfigMap key.\n\nBy following these steps, you should be able to resolve the issue of the Pod failing to read the config file at /app/config.json due to a missing or non-existent ConfigMap key."
  },
  {
    "case": "PodInitializing",
    "documentation": "1. Pod Initialization and Init Containers \n Init containers run before app containers in a Pod are started. If an Init container fails, Kubernetes retries it until it succeeds or the Pod is deleted.\n\n2. Service Discovery in Kubernetes \n Kubernetes uses DNS to allow Pods to discover Services. Each Service is assigned a DNS name, and Pods can use that name to communicate with the service.\n\n3. Endpoint Validation \n If a Pod cannot reach a service via DNS, ensure the service has endpoints: \n Kubectl get endpoints <service-name> \n Empty output means no matching Pods are available for the service selector\n\n4. Init Container Troubleshooting\n Use kubectl describe pod and kubectl logs <pod> -c <init-container> to understand why an Init container is stuck or failing. Network or DNS issues are common causes.\n\n5. Networking and Connectivity Testing \n You can use kubectl exec to test connectivity to a service. Use tools like nc or curl within the container.",
    "explanation": "The root cause of the issue is that the Pod is unable to reach the database service due to a short probe timeout and the use of a static IP instead of the service DNS name.\n\n1. Increase Probe Timeout Duration\n\nEdit the Pod's readiness and liveness probes to increase the timeout duration:\n\n```yaml\nspec:\n  containers:\n  - name: app-container\n    readinessProbe:\n      initialDelaySeconds: 5\n      periodSeconds: 10\n      timeoutSeconds: 15  # Increase timeout duration\n      successThreshold: 1\n      failureThreshold: 3\n    livenessProbe:\n      initialDelaySeconds: 10\n      periodSeconds: 15\n      timeoutSeconds: 20  # Increase timeout duration\n      successThreshold: 1\n      failureThreshold: 3\n```\n\n2. Replace Static IP with Service DNS Name\n\nUpdate the environment variable to use the service DNS name instead of a static IP:\n\n```yaml\nenv:\n- name: DB_HOST\n  value: \"database-service.default.svc.cluster.local\"  # Service DNS format\n```\n\n3. Verify Database Service Exists\n\nCheck if the database service exists in the default namespace:\n\n```bash\nkubectl get svc database-service -n default\n```\n\nIf the service is missing, create it with the appropriate selector and ports:\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: database-service\n  namespace: default\nspec:\n  selector:\n    app: database  # Must match database pod labels\n  ports:\n    - protocol: TCP\n      port: 5432\n      targetPort: 5432\n```\n\n4. Test Connectivity to Database Service\n\nExec into the Pod and test connectivity to the database service using tools like `nc` or `curl`:\n\n```bash\nkubectl exec -it <pod-name> -- /bin/bash\nnc -vz database-service.default.svc.cluster.local 5432\n```\n\nEnsure that the connection is successful, indicating that the Pod can reach the database service.\n\nBy following these steps, you should be able to resolve the Kubernetes issue related to probe timeouts and database service connectivity."
  },
  {
    "case": "CrashLoopBackOff",
    "documentation": "1. Service Discovery & DNS \n Pods should reference Services by their DNS name (<service>.<namespace>.svc.cluster.local), not static IPs. IPs are ephemeral in Kubernetes clusters.\n\n2. CrashLoopBackOff Definition \n A pod enters CrashLoopBackOff state when its containers repeatedly crash. Check logs with kubectl logs --previous to identify the root cause.\n\n3. Readiness Probe Best Practices \n For applications with slow startup: \n Set initialDelaySeconds longer than maximum initialization time \n timeoutSeconds should exceed expected request processing time\n\n4. Endpoint Verification \n Validate service-to-pod mapping with: \n kubectl get endpoints <service-name>  \n Empty results indicate no healthy pods match service selectors.\n\n5. DNS Resolution Troubleshooting \n Debug DNS issues from within pods using: \n kubectl exec -it <pod> -- nslookup <service> \n Failure indicates CoreDNS issues or missing service.",
    "explanation": "The root cause of the issue is that the pods are trying to connect to the database using a static IP address instead of the Service DNS name.\n\n1. Replace Static IP with Service DNS Name\n\n# Before\nenv: {\"DB_HOST\": \"10.0.0.5\"}\n\n# After\nenv:\n- name: DB_HOST\n  value: \"database-service.default.svc.cluster.local\"  # Service DNS format\n\n2. Verify Database Service Exists\n\nCheck if the database service exists in the default namespace:\n\n```bash\nkubectl get svc database-service -n default\n```\n\nIf the service is missing, create it with the following configuration:\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: database-service\n  namespace: default\nspec:\n  selector:\n    app: database  # Must match database pod labels\n  ports:\n    - protocol: TCP\n      port: 5432\n      targetPort: 5432\n```\n\n3. Check Endpoint Mapping\n\nVerify that the database service is correctly mapping to the database pods:\n\n```bash\nkubectl get endpoints database-service -n default\n```\n\nEnsure that the endpoints list contains the IP addresses of healthy database pods. Empty results indicate no healthy pods match the service selectors.\n\n4. Update Readiness Probe Configuration\n\nAdjust the readiness probe configuration to allow sufficient time for the database to start up:\n\n```yaml\nreadinessProbe:\n  httpGet:\n    path: /healthz\n    port: 5432\n  initialDelaySeconds: 30  # Adjust based on database startup time\n  timeoutSeconds: 10  # Exceed expected request processing time\n```\n\n5. Debug DNS Resolution\n\nIf the issue persists, debug DNS resolution from within the pods:\n\n```bash\nkubectl exec -it <pod-name> -- nslookup database-service.default.svc.cluster.local\n```\n\nEnsure that the DNS resolution is successful, failure may indicate CoreDNS issues or missing service configuration."
  },
  {
    "case": "ImagePullBackOff",
    "documentation": "1.  ImagePullBackOff Meaning \n The status ImagePullBackOff means that a container could not start because Kubernetes could not pull a container image (for reasons such as invalid image name, or pulling from a private registry without imagePullSecret). The BackOff part indicates that Kubernetes will keep trying to pull the image, with an increasing back-off delay.\n\n2. Causes of Image Pull Failures \n Kubernetes could not pull a container image for reasons such as invalid image name, or pulling from a private registry without imagePullSecret.\n\n3. Using imagePullSecrets for Private Registries \n Kubernetes supports specifying container image registry keys on a Pod. imagePullSecrets must all be in the same namespace as the Pod. The referenced Secrets must be of type kubernetes.io/dockercfg or kubernetes.io/dockerconfigjson. This is the recommended approach to run containers based on images in private registries.\n\n4. How imagePullPolicy Affects Image Pulling \n The imagePullPolicy for a container and the tag of the image affect when the kubelet attempts to pull (download) the specified image. \n If you would like to always force a pull, you can do one of the following: \n Set the imagePullPolicy of the container to Always. \n Omit the imagePullPolicy and use :latest as the tag for the image to use; Kubernetes will set the policy to Always when you submit the Pod. \n Omit the imagePullPolicy and the tag for the image to use; Kubernetes will set the policy to Always when you submit the Pod.",
    "explanation": "The root cause of the issue is that the container could not start due to Kubernetes not being able to pull the container image, possibly from a private registry without the necessary imagePullSecret.\n\n1. Check ImagePullBackOff Status\n\nRun the following command to check the status of the pod that is experiencing the ImagePullBackOff issue:\n```bash\nkubectl describe pod <pod_name> -n <namespace>\n```\nLook for events related to image pulling and check if there are any errors indicating why the image pull is failing.\n\n2. Verify ImagePullSecrets Configuration\n\nCheck if the Pod definition includes the necessary imagePullSecrets for pulling images from a private registry. Update the Pod definition if needed:\n```yaml\nspec:\n  imagePullSecrets:\n    - name: <image_pull_secret_name>\n```\nEnsure that the referenced Secret exists in the same namespace as the Pod and is of type kubernetes.io/dockercfg or kubernetes.io/dockerconfigjson.\n\n3. Adjust imagePullPolicy\n\nModify the imagePullPolicy of the container to force a pull of the image:\n```yaml\nspec:\n  containers:\n  - name: <container_name>\n    image: <image_name>\n    imagePullPolicy: Always\n```\nSetting the imagePullPolicy to Always ensures that Kubernetes will always attempt to pull the latest image version.\n\n4. Increase Probe Timeout\n\nAdjust the probe timeout settings to allow more time for the container to start up and establish connections with database dependencies:\n```yaml\nspec:\n  containers:\n  - name: <container_name>\n    readinessProbe:\n      initialDelaySeconds: 15\n      timeoutSeconds: 10  # Increase timeout as needed\n```\nIncreasing the timeoutSeconds value will give the container more time to complete the readiness probe before being marked as unhealthy.\n\n5. Restart the Pod\n\nAfter making the necessary changes to the Pod configuration, delete the existing Pod to trigger a new deployment:\n```bash\nkubectl delete pod <pod_name> -n <namespace>\n```\nKubernetes will create a new Pod with the updated configuration, which should now be able to pull the container image successfully and establish connections with database dependencies."
  },
  {
    "case": "OOMKilled",
    "documentation": "1. OOMKilled Pod Status \n A container is terminated and marked OOMKilled if it tries to use more memory than its limit. Kubernetes enforces these limits via the container runtime and the Linux kernel's OOM killer.\n\n2. Resource Limits in Pods \n You can specify memory and CPU limits in a Pod specification. The memory limit is enforced strictly: if a container exceeds it, it is killed.\n\n3. Configuring Node.js Memory via Environment Variables \n For Node.js applications, you can pass options like --max-old-space-size via the NODE_OPTIONS environment variable to raise or lower heap memory allocation.\n\n4. Container Exit Code 137\n Exit code 137 indicates that the container was terminated by the kernel due to an out-of-memory condition (SIGKILL).\n\n5. Monitoring Resource Usage with Metrics Server\n You can use the Kubernetes Metrics Server or other tools to inspect the actual memory usage of running containers before setting limits.",
    "explanation": "The root cause of the issue is that the Kubernetes Pod is running out of memory due to exceeding the memory limit set for the container.\n\n1. Increase Node.js Heap Memory Allocation\n   - Update the NODE_OPTIONS environment variable to increase the max-old-space-size to a higher value to allow more memory allocation for the Node.js application.\n   - Run the following command to update the environment variable:\n     ```bash\n     kubectl set env pod <pod-name> NODE_OPTIONS=\"--max-old-space-size=1024\"\n     ```\n   - This step increases the memory allocation available for the Node.js application, potentially resolving the out-of-memory issue.\n\n2. Monitor Memory Usage\n   - Use the Kubernetes Metrics Server or other monitoring tools to inspect the actual memory usage of the running containers.\n   - Run the following command to check the memory usage of the Pod:\n     ```bash\n     kubectl top pod <pod-name>\n     ```\n   - Look for memory usage metrics to ensure that the memory limits are appropriate for the workload.\n\n3. Adjust Memory Limits\n   - If monitoring reveals that the memory usage is consistently close to or exceeding the limits, consider adjusting the memory limits in the Pod specification.\n   - Update the Pod specification with higher memory limits as needed.\n   - This step ensures that the Pod has enough memory allocated to handle the workload without hitting OOM errors.\n\n4. Check for Resource Requests\n   - Ensure that resource requests are set appropriately in the Pod specification to prevent resource contention.\n   - Review the Pod specification to verify that resource requests for memory are set based on the workload requirements.\n   - Adjust resource requests if necessary to ensure that the Pod gets the required resources.\n\n5. Restart the Pod\n   - After making the necessary changes, restart the Pod to apply the new configurations.\n   - Run the following command to delete the Pod and let Kubernetes recreate it with the updated configurations:\n     ```bash\n     kubectl delete pod <pod-name>\n     ```\n   - This step ensures that the Pod starts fresh with the updated memory settings.\n\n6. Monitor Pod Behavior\n   - Monitor the Pod's behavior after the changes to ensure that it no longer experiences out-of-memory issues.\n   - Use logging and monitoring tools to track memory usage and performance of the Pod.\n   - If the issue persists, consider further optimizations or scaling options."
  }
]